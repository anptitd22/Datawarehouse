{"timestamp":"2025-06-21T12:00:11.062737","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-21T12:00:11.063881","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-21T12:00:12.076661Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:12.077689Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:12.078149Z","level":"info","event":"Current task name:dim_date_group.producer_kafka","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:12.078458Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:12.890395Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.926035Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.229688Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.230174Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.281822Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.284390Z","level":"error","event":"org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.287390Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-f83c5632-de9e-419c-bf34-3ed567a8ee22;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.288908Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.560202Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.609145Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.697937Z","level":"error","event":"\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.761292Z","level":"error","event":"\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.799118Z","level":"error","event":"\tfound org.apache.kafka#kafka-clients;2.8.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.832105Z","level":"error","event":"\tfound org.lz4#lz4-java;1.8.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.865319Z","level":"error","event":"\tfound org.xerial.snappy#snappy-java;1.1.8.4 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.898458Z","level":"error","event":"\tfound org.slf4j#slf4j-api;1.7.32 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.940040Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.976699Z","level":"error","event":"\tfound org.spark-project.spark#unused;1.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.047222Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.094450Z","level":"error","event":"\tfound commons-logging#commons-logging;1.1.3 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.122345Z","level":"error","event":"\tfound com.google.code.findbugs#jsr305;3.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.150486Z","level":"error","event":"\tfound org.apache.commons#commons-pool2;2.11.1 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.218968Z","level":"error","event":":: resolution report :: resolve 935ms :: artifacts dl 25ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.219926Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.220856Z","level":"error","event":"\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.221492Z","level":"error","event":"\tcommons-logging#commons-logging;1.1.3 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.221875Z","level":"error","event":"\torg.apache.commons#commons-pool2;2.11.1 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.222360Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.222686Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.222947Z","level":"error","event":"\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.223347Z","level":"error","event":"\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.223807Z","level":"error","event":"\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.224246Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.224682Z","level":"error","event":"\torg.lz4#lz4-java;1.8.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.225108Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.225556Z","level":"error","event":"\torg.slf4j#slf4j-api;1.7.32 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.225996Z","level":"error","event":"\torg.spark-project.spark#unused;1.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.226456Z","level":"error","event":"\torg.xerial.snappy#snappy-java;1.1.8.4 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.227074Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.227768Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.228449Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.229041Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.229949Z","level":"error","event":"\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.230589Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.242759Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-f83c5632-de9e-419c-bf34-3ed567a8ee22","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.244100Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.261933Z","level":"error","event":"\t0 artifacts copied, 14 already retrieved (0kB/18ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.631146Z","level":"error","event":"25/06/21 12:00:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.936621Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.938619Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.940151Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:19.327517Z","level":"error","event":"25/06/21 12:00:19 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:19.329276Z","level":"error","event":"25/06/21 12:00:19 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:22.955821","level":"info","event":"Thử lần 1/5 kiểm tra topic 'dim_date'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:23.015062","level":"info","event":"Topic 'dim_date' đã tồn tại.","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:23.316590","level":"info","event":"extract dim_date","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:25.004478","level":"info","event":"transform dim_date","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:25.004711","level":"info","event":"Extract và transform dim_date thành công","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:29.521478","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:29.948994","level":"info","event":"Batch 1: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:55.487555","level":"info","event":"Delivered to dim_date [0] at offset 1269","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488110","level":"info","event":"Delivered to dim_date [0] at offset 1270","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488371","level":"info","event":"Delivered to dim_date [0] at offset 1271","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488562","level":"info","event":"Delivered to dim_date [0] at offset 1272","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488655","level":"info","event":"Delivered to dim_date [0] at offset 1273","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488786","level":"info","event":"Delivered to dim_date [0] at offset 1274","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488861","level":"info","event":"Delivered to dim_date [0] at offset 1275","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.488990","level":"info","event":"Delivered to dim_date [0] at offset 1276","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.489148","level":"info","event":"Delivered to dim_date [0] at offset 1277","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.489240","level":"info","event":"Delivered to dim_date [0] at offset 1278","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.507636","level":"info","event":"Delivered to dim_date [1] at offset 1290","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.507882","level":"info","event":"Delivered to dim_date [1] at offset 1291","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.507957","level":"info","event":"Delivered to dim_date [1] at offset 1292","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508016","level":"info","event":"Delivered to dim_date [1] at offset 1293","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508074","level":"info","event":"Delivered to dim_date [1] at offset 1294","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508127","level":"info","event":"Delivered to dim_date [1] at offset 1295","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508180","level":"info","event":"Delivered to dim_date [1] at offset 1296","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508233","level":"info","event":"Delivered to dim_date [1] at offset 1297","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508289","level":"info","event":"Delivered to dim_date [1] at offset 1298","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508347","level":"info","event":"Delivered to dim_date [1] at offset 1299","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508405","level":"info","event":"Delivered to dim_date [1] at offset 1300","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508463","level":"info","event":"Delivered to dim_date [1] at offset 1301","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508533","level":"info","event":"Delivered to dim_date [2] at offset 1267","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508586","level":"info","event":"Delivered to dim_date [2] at offset 1268","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508645","level":"info","event":"Delivered to dim_date [2] at offset 1269","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508695","level":"info","event":"Delivered to dim_date [2] at offset 1270","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508796","level":"info","event":"Delivered to dim_date [2] at offset 1271","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508866","level":"info","event":"Delivered to dim_date [2] at offset 1272","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508926","level":"info","event":"Delivered to dim_date [2] at offset 1273","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.508984","level":"info","event":"Delivered to dim_date [2] at offset 1274","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.509054","level":"info","event":"Delivered to dim_date [2] at offset 1275","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.509112","level":"info","event":"Delivered to dim_date [2] at offset 1276","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.509170","level":"info","event":"Delivered to dim_date [2] at offset 1277","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510157","level":"info","event":"Delivered to dim_date [3] at offset 1207","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510328","level":"info","event":"Delivered to dim_date [3] at offset 1208","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510393","level":"info","event":"Delivered to dim_date [3] at offset 1209","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510454","level":"info","event":"Delivered to dim_date [3] at offset 1210","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510520","level":"info","event":"Delivered to dim_date [3] at offset 1211","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510583","level":"info","event":"Delivered to dim_date [3] at offset 1212","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510652","level":"info","event":"Delivered to dim_date [3] at offset 1213","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.510716","level":"info","event":"Delivered to dim_date [3] at offset 1214","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.511748","level":"info","event":"Delivered to dim_date [4] at offset 1241","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.511887","level":"info","event":"Delivered to dim_date [4] at offset 1242","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.511956","level":"info","event":"Delivered to dim_date [4] at offset 1243","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512025","level":"info","event":"Delivered to dim_date [4] at offset 1244","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512090","level":"info","event":"Delivered to dim_date [4] at offset 1245","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512154","level":"info","event":"Delivered to dim_date [4] at offset 1246","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512217","level":"info","event":"Delivered to dim_date [4] at offset 1247","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512279","level":"info","event":"Delivered to dim_date [4] at offset 1248","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.512342","level":"info","event":"Delivered to dim_date [4] at offset 1249","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:56.432908","level":"info","event":"extract dim_date","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:56.657155","level":"info","event":"transform dim_date","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:56.657469","level":"info","event":"Extract và transform dim_date thành công","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:58.746058","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:00:59.303037","level":"info","event":"Batch 2: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:01:27.453343Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 7:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 11:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 18:>                                                         (0 + 1) / 1]\r25/06/21 12:01:28 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 13)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.453877Z","level":"error","event":"org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.454214Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.454661Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.455435Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.456101Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.456481Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.456921Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.457218Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.457496Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.457745Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.458041Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.458350Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.458706Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.459081Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.459421Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.459652Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.460395Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.460852Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.461216Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.461452Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.461975Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.462302Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.462928Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.463849Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.464423Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.465161Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.466631Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.466999Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.468912Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.471126Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.471707Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.472104Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.478166Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.479086Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.479666Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.480066Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.480885Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.481304Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.481649Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.481951Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.482473Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.486197Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.487109Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.487637Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.488279Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.488834Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.489546Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.490030Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.490379Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.490699Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.491079Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.520343Z","level":"error","event":"25/06/21 12:01:28 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.521050Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.521399Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.521772Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.522183Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.522567Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.522887Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.523195Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.523507Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.523811Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.524108Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.524647Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.525101Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.525501Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.526083Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.529913Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.530912Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.531371Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.532455Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.532975Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.533290Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.534214Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.534889Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.535978Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.536562Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.536948Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.537321Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.537658Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.538061Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.538319Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.538656Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.538932Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.539178Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.539440Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.540950Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.541612Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.542053Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.542338Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.542585Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.542953Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.543357Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.543766Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.544220Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.544653Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.545000Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.545311Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.545601Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.545958Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.546247Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.546622Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.546956Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.547330Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.547649Z","level":"error","event":"25/06/21 12:01:28 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.548153Z","level":"error","event":"\r[Stage 18:>                                                         (0 + 0) / 1]\r25/06/21 12:01:28 ERROR Utils: Aborting task","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.548504Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.548818Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.549230Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.549512Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$2(PythonRDD.scala:265)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.549811Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.551670Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.552576Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:283)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.553304Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.553815Z","level":"error","event":"\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.554224Z","level":"error","event":"\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.554640Z","level":"error","event":"\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.555135Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.555537Z","level":"error","event":"\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.555929Z","level":"error","event":"Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.556554Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.556992Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.557393Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.562681Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.574345Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.576729Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.579457Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.581519Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.582371Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.583636Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.586737Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.587462Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.588510Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.589367Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.589781Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.590218Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.590581Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.591090Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.591429Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.591766Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.592075Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.592343Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.592700Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.593013Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.593337Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.593665Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.594117Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.594452Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.594946Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.595266Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.596224Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.597532Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.599640Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.311048","level":"error","event":"Lỗi trong quá trình gửi Kafka: An error occurred while calling o65.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:98)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:94)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$2(PythonRDD.scala:265)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:283)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:234)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)\n\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)\n\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)\n\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)\n\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 34 more\n","logger":"dags.etl_dim_date"}
{"timestamp":"2025-06-21T12:01:27.602333Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.603581Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.604276Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.605416Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.606308Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.606952Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.607371Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.607647Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.609043Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.609584Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.610938Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.611562Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.613369Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.614792Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.615245Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.616642Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.617796Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.618376Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.629963Z","level":"error","event":"Driver stacktrace:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.631618Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.632882Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.633229Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.633814Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.635902Z","level":"error","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.640983Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.644021Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.645901Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.646649Z","level":"error","event":"\tat scala.Option.foreach(Option.scala:437)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.651597Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.653487Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.659081Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.660022Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.661253Z","level":"error","event":"\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.662951Z","level":"error","event":"Caused by: org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.665908Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.666822Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.668439Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.669120Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.670201Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.670591Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.671003Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.671337Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.672302Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.672661Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.672948Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.673290Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.673558Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.673887Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.674190Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.674519Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.675515Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.675885Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.676185Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.676500Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.677264Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.677664Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.680658Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.683150Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.684022Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.684644Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.685031Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.685428Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.686718Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.688534Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.689086Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.689765Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.690166Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.691236Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.692098Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.692791Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.693703Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.694063Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.694402Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.694913Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.696320Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.696833Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.697791Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.698429Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.698897Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.699227Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.699969Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.700356Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.700733Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:27.700996Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.399832","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-06-21T12:01:28.461255Z","level":"info","event":"Task instance in success state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.462000Z","level":"info","event":" Previous state of the Task instance: running","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.462607Z","level":"info","event":"Task operator:<Task(PythonOperator): dim_date_group.producer_kafka>","chan":"stdout","logger":"task"}
