{"timestamp":"2025-06-21T12:00:10.707530","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-21T12:00:10.708274","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-21T12:00:11.316298Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:11.316945Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:11.317514Z","level":"info","event":"Current task name:dim_product_group.producer_kafka","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:11.317917Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:00:11.737601Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.487618Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.667255Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.668543Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.675441Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.676692Z","level":"error","event":"org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.677347Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-402f5356-8e7a-4c95-9adb-9837f727c56f;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.677781Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.914278Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:14.955950Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.133962Z","level":"error","event":"\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.241809Z","level":"error","event":"\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.352104Z","level":"error","event":"\tfound org.apache.kafka#kafka-clients;2.8.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.429773Z","level":"error","event":"\tfound org.lz4#lz4-java;1.8.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.470865Z","level":"error","event":"\tfound org.xerial.snappy#snappy-java;1.1.8.4 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.517605Z","level":"error","event":"\tfound org.slf4j#slf4j-api;1.7.32 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.599390Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.691346Z","level":"error","event":"\tfound org.spark-project.spark#unused;1.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.743614Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:15.855370Z","level":"error","event":"\tfound commons-logging#commons-logging;1.1.3 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.014044Z","level":"error","event":"\tfound com.google.code.findbugs#jsr305;3.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.091009Z","level":"error","event":"\tfound org.apache.commons#commons-pool2;2.11.1 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.215185Z","level":"error","event":":: resolution report :: resolve 1473ms :: artifacts dl 63ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.216121Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.217048Z","level":"error","event":"\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.217988Z","level":"error","event":"\tcommons-logging#commons-logging;1.1.3 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.223995Z","level":"error","event":"\torg.apache.commons#commons-pool2;2.11.1 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.224695Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.225228Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.225584Z","level":"error","event":"\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.227006Z","level":"error","event":"\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.228770Z","level":"error","event":"\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.229589Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.230165Z","level":"error","event":"\torg.lz4#lz4-java;1.8.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.230489Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.230739Z","level":"error","event":"\torg.slf4j#slf4j-api;1.7.32 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.231012Z","level":"error","event":"\torg.spark-project.spark#unused;1.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.231967Z","level":"error","event":"\torg.xerial.snappy#snappy-java;1.1.8.4 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.232926Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.233779Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.234376Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.234748Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.235100Z","level":"error","event":"\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.235528Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.236102Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-402f5356-8e7a-4c95-9adb-9837f727c56f","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.236543Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.248592Z","level":"error","event":"\t0 artifacts copied, 14 already retrieved (0kB/17ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:16.804259Z","level":"error","event":"25/06/21 12:00:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.233001Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.233928Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:17.234381Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:18.693881Z","level":"error","event":"25/06/21 12:00:18 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:22.955846","level":"info","event":"Thử lần 1/5 kiểm tra topic 'dim_product'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:23.010811","level":"info","event":"Topic 'dim_product' đã tồn tại.","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:23.412527","level":"info","event":"extract dim_product","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:25.033621","level":"info","event":"transform dim_product","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:25.901370","level":"info","event":"Extract và transform dim_product thành công","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:29.605708","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:30.068053","level":"info","event":"Batch 1: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:55.610327","level":"info","event":"Delivered to dim_product [0] at offset 1277","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.610626","level":"info","event":"Delivered to dim_product [0] at offset 1278","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.610711","level":"info","event":"Delivered to dim_product [0] at offset 1279","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.610784","level":"info","event":"Delivered to dim_product [0] at offset 1280","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.611821","level":"info","event":"Delivered to dim_product [0] at offset 1281","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.612625","level":"info","event":"Delivered to dim_product [0] at offset 1282","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.612873","level":"info","event":"Delivered to dim_product [0] at offset 1283","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.612964","level":"info","event":"Delivered to dim_product [0] at offset 1284","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613033","level":"info","event":"Delivered to dim_product [0] at offset 1285","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613137","level":"info","event":"Delivered to dim_product [1] at offset 1272","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613218","level":"info","event":"Delivered to dim_product [1] at offset 1273","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613288","level":"info","event":"Delivered to dim_product [1] at offset 1274","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613355","level":"info","event":"Delivered to dim_product [1] at offset 1275","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613419","level":"info","event":"Delivered to dim_product [1] at offset 1276","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613494","level":"info","event":"Delivered to dim_product [1] at offset 1277","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613565","level":"info","event":"Delivered to dim_product [1] at offset 1278","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613636","level":"info","event":"Delivered to dim_product [1] at offset 1279","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613702","level":"info","event":"Delivered to dim_product [1] at offset 1280","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613768","level":"info","event":"Delivered to dim_product [1] at offset 1281","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613833","level":"info","event":"Delivered to dim_product [2] at offset 1140","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613904","level":"info","event":"Delivered to dim_product [2] at offset 1141","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.613969","level":"info","event":"Delivered to dim_product [2] at offset 1142","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614034","level":"info","event":"Delivered to dim_product [2] at offset 1143","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614106","level":"info","event":"Delivered to dim_product [2] at offset 1144","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614174","level":"info","event":"Delivered to dim_product [2] at offset 1145","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614248","level":"info","event":"Delivered to dim_product [2] at offset 1146","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614322","level":"info","event":"Delivered to dim_product [3] at offset 1462","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614385","level":"info","event":"Delivered to dim_product [3] at offset 1463","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614449","level":"info","event":"Delivered to dim_product [3] at offset 1464","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614511","level":"info","event":"Delivered to dim_product [3] at offset 1465","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614576","level":"info","event":"Delivered to dim_product [3] at offset 1466","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614648","level":"info","event":"Delivered to dim_product [3] at offset 1467","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614707","level":"info","event":"Delivered to dim_product [3] at offset 1468","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614774","level":"info","event":"Delivered to dim_product [3] at offset 1469","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614851","level":"info","event":"Delivered to dim_product [3] at offset 1470","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614923","level":"info","event":"Delivered to dim_product [3] at offset 1471","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.614994","level":"info","event":"Delivered to dim_product [4] at offset 1136","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615064","level":"info","event":"Delivered to dim_product [4] at offset 1137","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615263","level":"info","event":"Delivered to dim_product [4] at offset 1138","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615351","level":"info","event":"Delivered to dim_product [4] at offset 1139","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615416","level":"info","event":"Delivered to dim_product [4] at offset 1140","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615485","level":"info","event":"Delivered to dim_product [4] at offset 1141","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615552","level":"info","event":"Delivered to dim_product [4] at offset 1142","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615619","level":"info","event":"Delivered to dim_product [4] at offset 1143","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615681","level":"info","event":"Delivered to dim_product [4] at offset 1144","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615742","level":"info","event":"Delivered to dim_product [4] at offset 1145","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615804","level":"info","event":"Delivered to dim_product [4] at offset 1146","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615888","level":"info","event":"Delivered to dim_product [4] at offset 1147","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.615959","level":"info","event":"Delivered to dim_product [4] at offset 1148","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:55.616027","level":"info","event":"Delivered to dim_product [4] at offset 1149","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:00:56.471284","level":"info","event":"extract dim_product","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:56.675014","level":"info","event":"transform dim_product","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:57.074375","level":"info","event":"Extract và transform dim_product thành công","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:01:28.320599Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 7:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 11:>                                                         (0 + 0) / 1]\r\r[Stage 11:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 18:>                                                         (0 + 1) / 1]\r25/06/21 12:01:28 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 13)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.321261Z","level":"error","event":"org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.321796Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.322213Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.322559Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.322897Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.323303Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.323646Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.323985Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.324281Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.324587Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.324961Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.325314Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.325658Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.325958Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.326372Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.326726Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.327126Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.327452Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.327735Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.328026Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.328393Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.328797Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.329695Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.330173Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.330515Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.330904Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.331434Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.331768Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.332071Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.332401Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.332712Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.332999Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.333338Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.333685Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.334009Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.334345Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.334740Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.336772Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.337902Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.338489Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.338923Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.339289Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.339984Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.340351Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.340826Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.341350Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.341922Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.342277Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.342592Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.342923Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.343222Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:00:58.648737","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:00:59.261804","level":"info","event":"Batch 2: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:01:28.366386Z","level":"error","event":"25/06/21 12:01:28 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.368338Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.369061Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.369530Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.369845Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.370121Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.370449Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.370769Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.371056Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.371336Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.371557Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.371785Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.371998Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.372250Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.372521Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.372771Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.373119Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.373435Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.373773Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.374052Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.374338Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.374625Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.374900Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.375178Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.375395Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.375641Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.375944Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.376165Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.376389Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.376581Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.376821Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.377122Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.377486Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.377789Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.378070Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.378325Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.378618Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.378854Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.379225Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.379531Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.379780Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.380009Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.380272Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.381034Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.381500Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.381874Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.382207Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.382497Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.382772Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.382968Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.383204Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.383696Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.384033Z","level":"error","event":"25/06/21 12:01:28 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.384370Z","level":"error","event":"25/06/21 12:01:28 ERROR Utils: Aborting task","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.384599Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.384814Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.385046Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.385295Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$2(PythonRDD.scala:265)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.385564Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.385817Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.386117Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:283)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.386432Z","level":"error","event":"\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.386709Z","level":"error","event":"\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.386963Z","level":"error","event":"\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.387251Z","level":"error","event":"\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.387588Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.387864Z","level":"error","event":"\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.388735Z","level":"error","event":"Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.389106Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.389392Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.389674Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.389932Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.390241Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.390503Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.390782Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.391076Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.391423Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.391705Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.391996Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.392244Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.392529Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.392828Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.393098Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.393375Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.393638Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.393876Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.394130Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.394389Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.394654Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.394928Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.395150Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.395364Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.395597Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.395850Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.396133Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.396418Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.396699Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.397006Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.397239Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.397460Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.397733Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.398067Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.399342Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.399877Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.400876Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.401209Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.401510Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.401793Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.402174Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.402512Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.402866Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.403199Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.403516Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.403790Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.404034Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.404321Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.404622Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.404931Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.405213Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.405516Z","level":"error","event":"Driver stacktrace:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.405804Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.406107Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.406437Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.406796Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.407290Z","level":"error","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.407766Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.408499Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.408983Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.409391Z","level":"error","event":"\tat scala.Option.foreach(Option.scala:437)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.409759Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.410097Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.410517Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.410823Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.411258Z","level":"error","event":"\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.411604Z","level":"error","event":"Caused by: org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.411913Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.412194Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.412417Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.412602Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.412916Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.413198Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.413411Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.413649Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.413910Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.414120Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.414323Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.414488Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.414714Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.415099Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.415405Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.415704Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.415908Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.416134Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.416373Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.416602Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.416855Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.230886","level":"error","event":"Lỗi trong quá trình gửi Kafka: An error occurred while calling o193.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:98)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:94)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$2(PythonRDD.scala:265)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1(PythonRDD.scala:283)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$toLocalIteratorAndServe$1$adapted(PythonRDD.scala:234)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:114)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:108)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:69)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:69)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 13) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)\n\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)\n\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.SocketTimeoutException: Read timed out\n\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)\n\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)\n\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)\n\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 34 more\n","logger":"dags.etl_dim_product"}
{"timestamp":"2025-06-21T12:01:28.402220","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-06-21T12:01:28.418067Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.418430Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.418946Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.419238Z","level":"error","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.419584Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.419899Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.420169Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.420438Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.420700Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.420932Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.421258Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.421653Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.421948Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.422240Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.422515Z","level":"error","event":"Caused by: java.net.SocketTimeoutException: Read timed out","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.422855Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:288)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.423136Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:314)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.423431Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.423798Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.424129Z","level":"error","event":"\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.424471Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:161)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.424804Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:128)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.425096Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.425365Z","level":"error","event":"\tat org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:73)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.425626Z","level":"error","event":"\tat org.postgresql.core.PGStream.receiveChar(PGStream.java:465)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.425861Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:589)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.426080Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.426496Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.427044Z","level":"error","event":"\t... 34 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.486230Z","level":"info","event":"Task instance in success state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.487442Z","level":"info","event":" Previous state of the Task instance: running","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:01:28.487938Z","level":"info","event":"Task operator:<Task(PythonOperator): dim_product_group.producer_kafka>","chan":"stdout","logger":"task"}
