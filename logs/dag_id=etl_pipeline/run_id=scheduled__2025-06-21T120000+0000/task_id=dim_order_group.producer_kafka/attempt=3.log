{"timestamp":"2025-06-21T12:17:33.842921","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-21T12:17:33.844002","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-21T12:17:34.212108Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:17:34.213294Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:17:34.214031Z","level":"info","event":"Current task name:dim_order_group.producer_kafka","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:17:34.214703Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:17:34.724507Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.743567Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.856094Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.856748Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.862365Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.863233Z","level":"error","event":"org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.864607Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-528fdbd8-daa2-4e99-8410-3f2b2400fbf8;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:36.865196Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.069166Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.100496Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.180244Z","level":"error","event":"\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.241435Z","level":"error","event":"\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.277698Z","level":"error","event":"\tfound org.apache.kafka#kafka-clients;2.8.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.317046Z","level":"error","event":"\tfound org.lz4#lz4-java;1.8.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.353151Z","level":"error","event":"\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.388553Z","level":"error","event":"\tfound org.slf4j#slf4j-api;1.7.32 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.440556Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.475949Z","level":"error","event":"\tfound org.spark-project.spark#unused;1.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.536974Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.580146Z","level":"error","event":"\tfound commons-logging#commons-logging;1.1.3 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.636030Z","level":"error","event":"\tfound com.google.code.findbugs#jsr305;3.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.667015Z","level":"error","event":"\tfound org.apache.commons#commons-pool2;2.11.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.732722Z","level":"error","event":":: resolution report :: resolve 826ms :: artifacts dl 42ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.733326Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.733903Z","level":"error","event":"\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.734411Z","level":"error","event":"\tcommons-logging#commons-logging;1.1.3 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.735199Z","level":"error","event":"\torg.apache.commons#commons-pool2;2.11.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.735759Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.736494Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.736957Z","level":"error","event":"\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.737430Z","level":"error","event":"\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.737830Z","level":"error","event":"\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.738197Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.738541Z","level":"error","event":"\torg.lz4#lz4-java;1.8.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.738919Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.739619Z","level":"error","event":"\torg.slf4j#slf4j-api;1.7.32 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.740047Z","level":"error","event":"\torg.spark-project.spark#unused;1.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.740515Z","level":"error","event":"\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.740868Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.741353Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.741817Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.742244Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.742843Z","level":"error","event":"\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.743283Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.747398Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-528fdbd8-daa2-4e99-8410-3f2b2400fbf8","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.748164Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:37.763506Z","level":"error","event":"\t0 artifacts copied, 14 already retrieved (0kB/16ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:38.309424Z","level":"error","event":"25/06/21 12:17:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:38.686258Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:38.687274Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:38.688344Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:17:50.564885","level":"info","event":"Thử lần 1/5 kiểm tra topic 'dim_order'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:17:50.849646","level":"info","event":"Đang tạo topic 'dim_order' với 5 partitions...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:17:52.122589","level":"info","event":"Đã tạo topic: dim_order","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:17:52.230489","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:17:56.008037","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:17:56.630998","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:03.538464","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:04.406635","level":"info","event":"Batch 1: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:09.297457","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:09.373627","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:09.448103","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:09.777383","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:10.167124","level":"info","event":"Batch 2: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:11.112610","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:11.200988","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:11.275475","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:11.811041","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:12.214648","level":"info","event":"Batch 3: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:12.820081","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:12.899136","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:12.959531","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:13.467523","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:13.716114","level":"info","event":"Batch 4: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:14.442025","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:14.599155","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:14.756205","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:15.282563","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:15.733830","level":"info","event":"Batch 5: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:16.631545","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:16.731687","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:16.816684","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:17.634384","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:18.374033","level":"info","event":"Batch 6: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:19.414707","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:19.497153","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:19.592398","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:20.256296","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:20.583859","level":"info","event":"Batch 7: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:21.226692","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:21.372795","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:21.459852","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:21.963064","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:22.400440","level":"info","event":"Batch 8: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:23.860628","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:23.987201","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:24.075296","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:38.907301","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:41.722685","level":"info","event":"Batch 9: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:42.460588","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:42.579180","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:42.636388","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:42.936320","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:43.275692","level":"info","event":"Batch 10: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:44.191480","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:44.285574","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:44.357805","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:44.726571","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:45.068997","level":"info","event":"Batch 11: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:46.031864","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:46.239277","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:46.296356","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:47.079834","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:47.419873","level":"info","event":"Batch 12: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:48.428402","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:48.507206","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:48.587231","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:49.277167","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:49.440815","level":"info","event":"Batch 13: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:50.314878","level":"info","event":"extract_dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:50.465156","level":"info","event":"transform dim_order","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:50.561626","level":"info","event":"Extract và transform dim_order thành công","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:18:52.724764","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:19:14.092631Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 1:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 88:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 89:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 96:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 144:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 147:>                                                        (0 + 1) / 1]\r25/06/21 12:19:14 ERROR Executor: Exception in task 0.0 in stage 147.0 (TID 107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.093939Z","level":"error","event":"org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.094506Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.095006Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.095475Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.095789Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.096055Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.096376Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.096707Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.097260Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.098116Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.098516Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.098846Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.099591Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.100293Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.100987Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.101484Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.102096Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.102455Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.102887Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.103250Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.103603Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.105381Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.106414Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.107202Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.107782Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.108264Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.108587Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.108929Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.109261Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.109624Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.110007Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.110251Z","level":"error","event":"Caused by: java.net.UnknownHostException: postgres_container","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.110565Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.110866Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.111144Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.111478Z","level":"error","event":"\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.111854Z","level":"error","event":"\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.112181Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.112483Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.112770Z","level":"error","event":"\t... 29 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.113079Z","level":"error","event":"25/06/21 12:19:14 WARN TaskSetManager: Lost task 0.0 in stage 147.0 (TID 107) (dbf90a04ae06 executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.113357Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.113649Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.114176Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.114684Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.114989Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.115335Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.142616Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.143141Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.143612Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.143975Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.144270Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.144589Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.144928Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.145293Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.146165Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.146759Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.147251Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.147841Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.148240Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.148597Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.148921Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.149202Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.149580Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.149906Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.150287Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.150572Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.150938Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.151562Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.152170Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.152525Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.152844Z","level":"error","event":"Caused by: java.net.UnknownHostException: postgres_container","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.153138Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.153663Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.154021Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.154345Z","level":"error","event":"\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.154726Z","level":"error","event":"\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.155068Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.155373Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.155663Z","level":"error","event":"\t... 29 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.155929Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.156239Z","level":"error","event":"25/06/21 12:19:14 ERROR TaskSetManager: Task 0 in stage 147.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:19:13.067340","level":"error","event":"Lỗi trong quá trình gửi Kafka: An error occurred while calling o305.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 147.0 failed 1 times, most recent failure: Lost task 0.0 in stage 147.0 (TID 107) (dbf90a04ae06 executor driver): org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.UnknownHostException: postgres_container\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.UnknownHostException: postgres_container\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 29 more\n","logger":"dags.etl_dim_order"}
{"timestamp":"2025-06-21T12:19:14.397908","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-06-21T12:19:14.466776Z","level":"info","event":"Task instance in success state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.468097Z","level":"info","event":" Previous state of the Task instance: running","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:19:14.468879Z","level":"info","event":"Task operator:<Task(PythonOperator): dim_order_group.producer_kafka>","chan":"stdout","logger":"task"}
