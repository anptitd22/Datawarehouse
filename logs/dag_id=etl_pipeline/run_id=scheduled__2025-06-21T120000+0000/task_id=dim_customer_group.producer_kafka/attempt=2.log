{"timestamp":"2025-06-21T12:04:30.922709","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-21T12:04:30.923504","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-21T12:04:31.491751Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:04:31.492929Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:04:31.493496Z","level":"info","event":"Current task name:dim_customer_group.producer_kafka","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:04:31.493923Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:04:32.136951Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.771407Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.939324Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.975280Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.976164Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.976735Z","level":"error","event":"org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.977316Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-b793e2e0-cd88-4257-8355-d94e30ff6f02;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:35.977744Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.277970Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.328056Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.436748Z","level":"error","event":"\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.513025Z","level":"error","event":"\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.570225Z","level":"error","event":"\tfound org.apache.kafka#kafka-clients;2.8.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.607429Z","level":"error","event":"\tfound org.lz4#lz4-java;1.8.0 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.649592Z","level":"error","event":"\tfound org.xerial.snappy#snappy-java;1.1.8.4 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.696024Z","level":"error","event":"\tfound org.slf4j#slf4j-api;1.7.32 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.739916Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.771245Z","level":"error","event":"\tfound org.spark-project.spark#unused;1.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.830975Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:36.990601Z","level":"error","event":"\tfound commons-logging#commons-logging;1.1.3 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.149390Z","level":"error","event":"\tfound com.google.code.findbugs#jsr305;3.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.332165Z","level":"error","event":"\tfound org.apache.commons#commons-pool2;2.11.1 in spark-list","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.520301Z","level":"error","event":":: resolution report :: resolve 1504ms :: artifacts dl 59ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.522057Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.525345Z","level":"error","event":"\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.526226Z","level":"error","event":"\tcommons-logging#commons-logging;1.1.3 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.526790Z","level":"error","event":"\torg.apache.commons#commons-pool2;2.11.1 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.527202Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.527665Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.527966Z","level":"error","event":"\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.528237Z","level":"error","event":"\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.528942Z","level":"error","event":"\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.529591Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.530976Z","level":"error","event":"\torg.lz4#lz4-java;1.8.0 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.531495Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.531838Z","level":"error","event":"\torg.slf4j#slf4j-api;1.7.32 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.532125Z","level":"error","event":"\torg.spark-project.spark#unused;1.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.532385Z","level":"error","event":"\torg.xerial.snappy#snappy-java;1.1.8.4 from spark-list in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.532647Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.532987Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.542188Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.543039Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.543671Z","level":"error","event":"\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.544698Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.553444Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-b793e2e0-cd88-4257-8355-d94e30ff6f02","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.554479Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:37.572114Z","level":"error","event":"\t0 artifacts copied, 14 already retrieved (0kB/17ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:38.691528Z","level":"error","event":"25/06/21 12:04:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:39.372027Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:39.374188Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:39.375328Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:42.827320Z","level":"error","event":"25/06/21 12:04:42 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:42.863922Z","level":"error","event":"25/06/21 12:04:42 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:04:50.911722","level":"info","event":"Thử lần 1/5 kiểm tra topic 'dim_customer'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:04:50.966033","level":"info","event":"Topic 'dim_customer' đã tồn tại.","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:04:51.168268","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:04:53.235284","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:04:54.232917","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:04:59.114124","level":"info","event":"Tổng số bản ghi cần gửi: 50","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:04:59.773669","level":"info","event":"Batch 1: 50 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:05:14.842479","level":"info","event":"Delivered to dim_customer [2] at offset 1073","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.846988","level":"info","event":"Delivered to dim_customer [2] at offset 1074","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847449","level":"info","event":"Delivered to dim_customer [2] at offset 1075","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847582","level":"info","event":"Delivered to dim_customer [2] at offset 1076","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847680","level":"info","event":"Delivered to dim_customer [2] at offset 1077","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847755","level":"info","event":"Delivered to dim_customer [2] at offset 1078","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847816","level":"info","event":"Delivered to dim_customer [2] at offset 1079","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847880","level":"info","event":"Delivered to dim_customer [2] at offset 1080","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.847948","level":"info","event":"Delivered to dim_customer [2] at offset 1081","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848035","level":"info","event":"Delivered to dim_customer [3] at offset 1158","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848102","level":"info","event":"Delivered to dim_customer [3] at offset 1159","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848168","level":"info","event":"Delivered to dim_customer [3] at offset 1160","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848320","level":"info","event":"Delivered to dim_customer [3] at offset 1161","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848425","level":"info","event":"Delivered to dim_customer [3] at offset 1162","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848501","level":"info","event":"Delivered to dim_customer [3] at offset 1163","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848566","level":"info","event":"Delivered to dim_customer [3] at offset 1164","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848662","level":"info","event":"Delivered to dim_customer [3] at offset 1165","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848774","level":"info","event":"Delivered to dim_customer [3] at offset 1166","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848864","level":"info","event":"Delivered to dim_customer [3] at offset 1167","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.848956","level":"info","event":"Delivered to dim_customer [3] at offset 1168","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.856349","level":"info","event":"Delivered to dim_customer [4] at offset 1123","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.856716","level":"info","event":"Delivered to dim_customer [4] at offset 1124","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.856833","level":"info","event":"Delivered to dim_customer [4] at offset 1125","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.856931","level":"info","event":"Delivered to dim_customer [4] at offset 1126","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.857011","level":"info","event":"Delivered to dim_customer [4] at offset 1127","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.857089","level":"info","event":"Delivered to dim_customer [4] at offset 1128","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.857174","level":"info","event":"Delivered to dim_customer [4] at offset 1129","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.857277","level":"info","event":"Delivered to dim_customer [4] at offset 1130","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.857354","level":"info","event":"Delivered to dim_customer [4] at offset 1131","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.861396","level":"info","event":"Delivered to dim_customer [0] at offset 1255","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.862038","level":"info","event":"Delivered to dim_customer [0] at offset 1256","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.862278","level":"info","event":"Delivered to dim_customer [0] at offset 1257","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.862370","level":"info","event":"Delivered to dim_customer [0] at offset 1258","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.862441","level":"info","event":"Delivered to dim_customer [0] at offset 1259","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.862528","level":"info","event":"Delivered to dim_customer [0] at offset 1260","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.871871","level":"info","event":"Delivered to dim_customer [1] at offset 1169","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872318","level":"info","event":"Delivered to dim_customer [1] at offset 1170","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872459","level":"info","event":"Delivered to dim_customer [1] at offset 1171","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872583","level":"info","event":"Delivered to dim_customer [1] at offset 1172","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872672","level":"info","event":"Delivered to dim_customer [1] at offset 1173","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872744","level":"info","event":"Delivered to dim_customer [1] at offset 1174","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.872815","level":"info","event":"Delivered to dim_customer [1] at offset 1175","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873020","level":"info","event":"Delivered to dim_customer [1] at offset 1176","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873120","level":"info","event":"Delivered to dim_customer [1] at offset 1177","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873195","level":"info","event":"Delivered to dim_customer [1] at offset 1178","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873270","level":"info","event":"Delivered to dim_customer [1] at offset 1179","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873429","level":"info","event":"Delivered to dim_customer [1] at offset 1180","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873509","level":"info","event":"Delivered to dim_customer [1] at offset 1181","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873664","level":"info","event":"Delivered to dim_customer [1] at offset 1182","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:14.873743","level":"info","event":"Delivered to dim_customer [1] at offset 1183","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T12:05:15.678363","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:05:15.774240","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:05:16.360522","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:06:23.606334Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 12:>                                                         (0 + 1) / 1]\r25/06/21 12:05:36 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 9)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.607010Z","level":"error","event":"org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.607488Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.607798Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.608093Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.608672Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.609044Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.609491Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.609691Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.610009Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.610284Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.610589Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.612076Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.612500Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.612935Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.613204Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.613538Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.613794Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.614735Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.615019Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.615269Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.615565Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.615825Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.616812Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.617182Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.617441Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.617690Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.617940Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.618161Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.618419Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.619594Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.620050Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.620367Z","level":"error","event":"Caused by: java.net.UnknownHostException: postgres_container","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.620684Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.620965Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.621274Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.622708Z","level":"error","event":"\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.623250Z","level":"error","event":"\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.623650Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.624001Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.624298Z","level":"error","event":"\t... 29 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.624610Z","level":"error","event":"25/06/21 12:05:36 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 9) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.624894Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.625161Z","level":"error","event":"\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.625466Z","level":"error","event":"\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.625832Z","level":"error","event":"\tat org.postgresql.Driver.makeConnection(Driver.java:444)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.626201Z","level":"error","event":"\tat org.postgresql.Driver.connect(Driver.java:297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.626542Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.626845Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.627157Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.679811Z","level":"info","event":"Task instance in success state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.680155Z","level":"info","event":" Previous state of the Task instance: running","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.680492Z","level":"info","event":"Task operator:<Task(PythonOperator): dim_customer_group.producer_kafka>","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.680972Z","level":"error","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.681255Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.681490Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.681752Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.682056Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.682320Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.682648Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.682942Z","level":"error","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.683240Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.683529Z","level":"error","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.683789Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.684014Z","level":"error","event":"\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.684236Z","level":"error","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.684596Z","level":"error","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.684865Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.685239Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.685572Z","level":"error","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.685880Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.686184Z","level":"error","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.686458Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.686888Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.688177Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.688545Z","level":"error","event":"Caused by: java.net.UnknownHostException: postgres_container","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.688941Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.689199Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.689481Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.689829Z","level":"error","event":"\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.690109Z","level":"error","event":"\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.690365Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.690615Z","level":"error","event":"\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.690852Z","level":"error","event":"\t... 29 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.691105Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:06:23.691388Z","level":"error","event":"25/06/21 12:05:36 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T12:05:36.970018","level":"error","event":"Lỗi khi gửi Kafka batch 2: An error occurred while calling o203.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 9) (b67c5166c6aa executor driver): org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.UnknownHostException: postgres_container\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:260)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.net.UnknownHostException: postgres_container\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\t... 29 more\n","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T12:06:23.209732","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
