{"timestamp":"2025-06-21T09:08:27.655895","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-21T09:08:27.657035","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-21T09:08:28.643804Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:08:28.644784Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:08:28.645201Z","level":"info","event":"Current task name:dim_customer_group.producer_kafka","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:08:28.645711Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:08:28.646099Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.177409Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.333085Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.333919Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.347204Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.349347Z","level":"error","event":"org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.353474Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-27e087c8-c013-4b28-b90f-cb7ab831df0a;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.354223Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.607139Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.643182Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.738333Z","level":"error","event":"\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.818022Z","level":"error","event":"\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.864316Z","level":"error","event":"\tfound org.apache.kafka#kafka-clients;2.8.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.899846Z","level":"error","event":"\tfound org.lz4#lz4-java;1.8.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.940862Z","level":"error","event":"\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:31.980717Z","level":"error","event":"\tfound org.slf4j#slf4j-api;1.7.32 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.062737Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.109331Z","level":"error","event":"\tfound org.spark-project.spark#unused;1.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.167449Z","level":"error","event":"\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.230436Z","level":"error","event":"\tfound commons-logging#commons-logging;1.1.3 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.274511Z","level":"error","event":"\tfound com.google.code.findbugs#jsr305;3.0.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.329074Z","level":"error","event":"\tfound org.apache.commons#commons-pool2;2.11.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.458590Z","level":"error","event":":: resolution report :: resolve 1008ms :: artifacts dl 94ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.459218Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.460297Z","level":"error","event":"\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.460851Z","level":"error","event":"\tcommons-logging#commons-logging;1.1.3 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.461157Z","level":"error","event":"\torg.apache.commons#commons-pool2;2.11.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.461858Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.462296Z","level":"error","event":"\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.462756Z","level":"error","event":"\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.463039Z","level":"error","event":"\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.463268Z","level":"error","event":"\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.463671Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.464022Z","level":"error","event":"\torg.lz4#lz4-java;1.8.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.464356Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.464609Z","level":"error","event":"\torg.slf4j#slf4j-api;1.7.32 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.464842Z","level":"error","event":"\torg.spark-project.spark#unused;1.0.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.465070Z","level":"error","event":"\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.465368Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.465821Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.466364Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.466727Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.467105Z","level":"error","event":"\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.467415Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.473290Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-27e087c8-c013-4b28-b90f-cb7ab831df0a","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.474672Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.492123Z","level":"error","event":"\t0 artifacts copied, 14 already retrieved (0kB/18ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:32.985451Z","level":"error","event":"25/06/21 09:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:33.399900Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:33.401020Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:33.401952Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:36.254140Z","level":"error","event":"25/06/21 09:08:36 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:36.254910Z","level":"error","event":"25/06/21 09:08:36 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:08:58.076442","level":"info","event":"Thử lần 1/5 kiểm tra topic 'dim_customer'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:03.268395","level":"warning","event":"⚠Lỗi khi kiểm tra/tạo topic 'dim_customer': KafkaError{code=_TRANSPORT,val=-195,str=\"Failed to get metadata: Local: Broker transport failure\"}","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:03.376779","level":"info","event":"Đợi 3 giây rồi thử lại...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:08.506818","level":"info","event":"Thử lần 2/5 kiểm tra topic 'dim_customer'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:13.743922","level":"warning","event":"⚠Lỗi khi kiểm tra/tạo topic 'dim_customer': KafkaError{code=_TRANSPORT,val=-195,str=\"Failed to get metadata: Local: Broker transport failure\"}","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:13.748365","level":"info","event":"Đợi 3 giây rồi thử lại...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:14.988457Z","level":"error","event":"%3|1750496948.508|FAIL|rdkafka#producer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Failed to resolve 'kafka:9092': Temporary failure in name resolution (after 10426ms in state CONNECT)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:09:14.991247Z","level":"error","event":"%3|1750496948.508|FAIL|rdkafka#producer-2| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Failed to resolve 'kafka:9092': Temporary failure in name resolution (after 10425ms in state CONNECT)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:09:16.024171","level":"info","event":"Thử lần 3/5 kiểm tra topic 'dim_customer'...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:16.034338","level":"info","event":"Đang tạo topic 'dim_customer' với 5 partitions...","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:17.526918","level":"info","event":"Đã tạo topic: dim_customer","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:09:17.533662","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:11:37.193888","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:11:45.929362","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:12:58.244885Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/21 09:12:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:12:58.245549Z","level":"error","event":"25/06/21 09:12:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:12:58.246110Z","level":"error","event":"25/06/21 09:12:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:12:58.247629Z","level":"error","event":"25/06/21 09:12:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:13:06.566020Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/21 09:13:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:13:21.542825Z","level":"error","event":"25/06/21 09:13:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:13:42.389429Z","level":"error","event":"25/06/21 09:13:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:13:50.003074Z","level":"error","event":"25/06/21 09:13:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:14:05.001959Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/21 09:14:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-21T09:14:22.431235","level":"info","event":"Tổng số bản ghi cần gửi: 20","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:23.331568","level":"info","event":"Batch 1: 20 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:24.527452","level":"info","event":"Delivered to dim_customer [0] at offset 0","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.527726","level":"info","event":"Delivered to dim_customer [0] at offset 1","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.528895","level":"info","event":"Delivered to dim_customer [1] at offset 0","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.529092","level":"info","event":"Delivered to dim_customer [1] at offset 1","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.529234","level":"info","event":"Delivered to dim_customer [1] at offset 2","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.529357","level":"info","event":"Delivered to dim_customer [1] at offset 3","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.529428","level":"info","event":"Delivered to dim_customer [1] at offset 4","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.529481","level":"info","event":"Delivered to dim_customer [1] at offset 5","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.531749","level":"info","event":"Delivered to dim_customer [2] at offset 0","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.532025","level":"info","event":"Delivered to dim_customer [2] at offset 1","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.533799","level":"info","event":"Delivered to dim_customer [3] at offset 0","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.534013","level":"info","event":"Delivered to dim_customer [3] at offset 1","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.534092","level":"info","event":"Delivered to dim_customer [3] at offset 2","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.534158","level":"info","event":"Delivered to dim_customer [3] at offset 3","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.534227","level":"info","event":"Delivered to dim_customer [3] at offset 4","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.534290","level":"info","event":"Delivered to dim_customer [3] at offset 5","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.535431","level":"info","event":"Delivered to dim_customer [4] at offset 0","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.535639","level":"info","event":"Delivered to dim_customer [4] at offset 1","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.535720","level":"info","event":"Delivered to dim_customer [4] at offset 2","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.535779","level":"info","event":"Delivered to dim_customer [4] at offset 3","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:24.852249","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:24.904554","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:25.233672","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:25.860001","level":"info","event":"Tổng số bản ghi cần gửi: 20","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:26.182318","level":"info","event":"Batch 2: 20 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:26.574286","level":"info","event":"Delivered to dim_customer [3] at offset 6","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.574580","level":"info","event":"Delivered to dim_customer [3] at offset 7","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.578580","level":"info","event":"Delivered to dim_customer [4] at offset 4","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.579613","level":"info","event":"Delivered to dim_customer [4] at offset 5","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.586292","level":"info","event":"Delivered to dim_customer [4] at offset 6","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.586740","level":"info","event":"Delivered to dim_customer [0] at offset 2","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.586878","level":"info","event":"Delivered to dim_customer [0] at offset 3","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587203","level":"info","event":"Delivered to dim_customer [0] at offset 4","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587333","level":"info","event":"Delivered to dim_customer [1] at offset 6","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587401","level":"info","event":"Delivered to dim_customer [1] at offset 7","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587472","level":"info","event":"Delivered to dim_customer [1] at offset 8","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587534","level":"info","event":"Delivered to dim_customer [1] at offset 9","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587610","level":"info","event":"Delivered to dim_customer [1] at offset 10","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587676","level":"info","event":"Delivered to dim_customer [1] at offset 11","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587743","level":"info","event":"Delivered to dim_customer [1] at offset 12","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.587807","level":"info","event":"Delivered to dim_customer [2] at offset 2","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.588077","level":"info","event":"Delivered to dim_customer [2] at offset 3","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.588200","level":"info","event":"Delivered to dim_customer [2] at offset 4","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.588273","level":"info","event":"Delivered to dim_customer [2] at offset 5","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:26.588338","level":"info","event":"Delivered to dim_customer [2] at offset 6","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:27.033693","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:27.071712","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:27.403652","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:27.866825","level":"info","event":"Tổng số bản ghi cần gửi: 20","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:28.272534","level":"info","event":"Batch 3: 20 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:30.607295","level":"info","event":"Delivered to dim_customer [0] at offset 5","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608456","level":"info","event":"Delivered to dim_customer [0] at offset 6","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608677","level":"info","event":"Delivered to dim_customer [0] at offset 7","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608757","level":"info","event":"Delivered to dim_customer [0] at offset 8","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608814","level":"info","event":"Delivered to dim_customer [0] at offset 9","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608857","level":"info","event":"Delivered to dim_customer [0] at offset 10","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.608900","level":"info","event":"Delivered to dim_customer [0] at offset 11","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.610663","level":"info","event":"Delivered to dim_customer [1] at offset 13","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.610845","level":"info","event":"Delivered to dim_customer [1] at offset 14","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.615636","level":"info","event":"Delivered to dim_customer [2] at offset 7","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.615959","level":"info","event":"Delivered to dim_customer [2] at offset 8","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.616045","level":"info","event":"Delivered to dim_customer [2] at offset 9","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.618320","level":"info","event":"Delivered to dim_customer [3] at offset 8","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.618502","level":"info","event":"Delivered to dim_customer [3] at offset 9","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.618574","level":"info","event":"Delivered to dim_customer [3] at offset 10","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.618637","level":"info","event":"Delivered to dim_customer [3] at offset 11","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.624530","level":"info","event":"Delivered to dim_customer [4] at offset 7","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.624734","level":"info","event":"Delivered to dim_customer [4] at offset 8","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.624808","level":"info","event":"Delivered to dim_customer [4] at offset 9","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:30.624869","level":"info","event":"Delivered to dim_customer [4] at offset 10","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:31.385419","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:31.526493","level":"info","event":"transform_dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:31.787129","level":"info","event":"extract_transform success","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:32.687297","level":"info","event":"Tổng số bản ghi cần gửi: 20","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:37.205037","level":"info","event":"Batch 4: 20 bản ghi. Đang gửi lên Kafka...","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:14:37.822854","level":"info","event":"Delivered to dim_customer [1] at offset 15","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.826762","level":"info","event":"Delivered to dim_customer [1] at offset 16","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.826923","level":"info","event":"Delivered to dim_customer [1] at offset 17","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827051","level":"info","event":"Delivered to dim_customer [1] at offset 18","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827122","level":"info","event":"Delivered to dim_customer [1] at offset 19","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827188","level":"info","event":"Delivered to dim_customer [1] at offset 20","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827251","level":"info","event":"Delivered to dim_customer [1] at offset 21","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827324","level":"info","event":"Delivered to dim_customer [2] at offset 10","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827386","level":"info","event":"Delivered to dim_customer [2] at offset 11","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827448","level":"info","event":"Delivered to dim_customer [2] at offset 12","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827528","level":"info","event":"Delivered to dim_customer [2] at offset 13","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827592","level":"info","event":"Delivered to dim_customer [3] at offset 12","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827664","level":"info","event":"Delivered to dim_customer [3] at offset 13","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827726","level":"info","event":"Delivered to dim_customer [3] at offset 14","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827798","level":"info","event":"Delivered to dim_customer [3] at offset 15","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827869","level":"info","event":"Delivered to dim_customer [3] at offset 16","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.827934","level":"info","event":"Delivered to dim_customer [3] at offset 17","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.828708","level":"info","event":"Delivered to dim_customer [0] at offset 12","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.828870","level":"info","event":"Delivered to dim_customer [0] at offset 13","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:14:37.828934","level":"info","event":"Delivered to dim_customer [0] at offset 14","logger":"kafka.build_kafka"}
{"timestamp":"2025-06-21T09:15:31.162598","level":"info","event":"extract dim_customer","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:16:08.235719Z","level":"info","event":"Task instance in success state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:16:08.236380Z","level":"info","event":" Previous state of the Task instance: running","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:16:08.237577Z","level":"info","event":"Task operator:<Task(PythonOperator): dim_customer_group.producer_kafka>","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-21T09:16:05.555074","level":"error","event":"Lỗi khi gửi Kafka batch 5: An error occurred while calling o370.load.\n: org.postgresql.util.PSQLException: The connection attempt failed.\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:67)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:243)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:38)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)\n\t\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\t\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\t\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\t\tat org.postgresql.Driver.connect(Driver.java:297)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:67)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:243)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:38)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\nCaused by: java.net.UnknownHostException: postgres_container\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat org.postgresql.core.PGStream.createSocket(PGStream.java:243)\n\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)\n\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)\n\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)\n\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:263)\n\tat org.postgresql.Driver.makeConnection(Driver.java:444)\n\tat org.postgresql.Driver.connect(Driver.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:67)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:243)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:38)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 21 more\n","logger":"dags.etl_dim_customer"}
{"timestamp":"2025-06-21T09:16:07.274467","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-06-21T09:16:07.888069Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 1:>                                                          (0 + 1) / 1]\r\r[Stage 3:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 7:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 29:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 37:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 41:>                                                         (0 + 1) / 1]\r\r[Stage 43:>                                                         (0 + 1) / 1]","chan":"stderr","logger":"task"}
