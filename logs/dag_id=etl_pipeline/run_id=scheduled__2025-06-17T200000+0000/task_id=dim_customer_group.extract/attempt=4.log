{"timestamp":"2025-06-17T20:09:39.324450","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-17T20:09:39.324962","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-17T20:09:39.573429Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:09:39.574348Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:09:39.574736Z","level":"info","event":"Current task name:dim_customer_group.extract","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:09:39.575043Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:09:40.021513Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.380527Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.461958Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.462795Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.467081Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.468454Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-3d858d20-3c81-46a8-bb9e-7b569283b046;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.468936Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.566568Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.588580Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.603074Z","level":"error","event":":: resolution report :: resolve 130ms :: artifacts dl 4ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.603844Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.604465Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.604859Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.605232Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.605639Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.606358Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.606751Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.607066Z","level":"error","event":"\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.607367Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.607771Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-3d858d20-3c81-46a8-bb9e-7b569283b046","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.608153Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.614495Z","level":"error","event":"\t0 artifacts copied, 2 already retrieved (0kB/7ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:41.833335Z","level":"error","event":"25/06/17 20:09:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:42.104449Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:42.105369Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:09:42.105887Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:11.158674Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:10:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:26.156822Z","level":"error","event":"25/06/17 20:10:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:41.669310Z","level":"error","event":"25/06/17 20:10:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:56.669457Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:10:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:11.796482Z","level":"error","event":"25/06/17 20:11:11 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:26.796428Z","level":"error","event":"25/06/17 20:11:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:41.947236Z","level":"error","event":"25/06/17 20:11:41 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:56.946741Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:11:56 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:12.079147Z","level":"error","event":"25/06/17 20:12:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:27.079469Z","level":"error","event":"25/06/17 20:12:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:42.210101Z","level":"error","event":"25/06/17 20:12:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:57.210049Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:12:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:12.376810Z","level":"error","event":"25/06/17 20:13:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:27.376926Z","level":"error","event":"25/06/17 20:13:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:42.525343Z","level":"error","event":"25/06/17 20:13:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:57.525104Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:13:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:12.670043Z","level":"error","event":"25/06/17 20:14:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:27.670113Z","level":"error","event":"25/06/17 20:14:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:42.821516Z","level":"error","event":"25/06/17 20:14:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:57.830907Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:14:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:12.977428Z","level":"error","event":"25/06/17 20:15:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:27.977286Z","level":"error","event":"25/06/17 20:15:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:43.117176Z","level":"error","event":"25/06/17 20:15:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:58.117296Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:15:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:13.252992Z","level":"error","event":"25/06/17 20:16:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:28.253041Z","level":"error","event":"25/06/17 20:16:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:43.418410Z","level":"error","event":"25/06/17 20:16:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:58.418589Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:16:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:13.572411Z","level":"error","event":"25/06/17 20:17:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:23.677473Z","level":"error","event":"Server indicated the task shouldn't be running anymore. Terminating process","detail":{"detail":{"reason":"not_running","message":"TI is no longer in the running state and task should terminate","current_state":"failed"}},"logger":"task"}
{"timestamp":"2025-06-17T20:17:23.762669Z","level":"error","event":"Task killed!","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.472336Z","level":"error","event":"25/06/17 20:17:25 ERROR FileFormatWriter: Aborting job 9d9e000a-1c87-447f-a49f-30a5aa80ecff.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.472757Z","level":"error","event":"org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.473498Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.474238Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.474756Z","level":"error","event":"\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.475498Z","level":"error","event":"\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.476249Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.478419Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.479268Z","level":"error","event":"\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.479751Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.480948Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.481879Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.482771Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.483715Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.484309Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.484791Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.485157Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$36(SparkContext.scala:704)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.485556Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.486739Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.487534Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.488274Z","level":"error","event":"\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.488880Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.490097Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.490736Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.491041Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.491399Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.491722Z","level":"error","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.492063Z","level":"error","event":"\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.492316Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.492607Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.493284Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.493801Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.494290Z","level":"error","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.494867Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.495342Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.495742Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.496633Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.497697Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.498163Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.498595Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.498974Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.499331Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.500418Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.501010Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.501441Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.501834Z","level":"error","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.502340Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.502771Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.503093Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.503413Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.503768Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.504096Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.504414Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.504661Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.504999Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.505307Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.505587Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.505836Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.506104Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.506366Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.506645Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.506951Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.507201Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.507525Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.507853Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.508243Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.508567Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.508877Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.509158Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.509429Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.509642Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.510310Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.510657Z","level":"error","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.510976Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.511258Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.512124Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.512608Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.512966Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.513218Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.513890Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.514457Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.514831Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.515188Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.515436Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.515730Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.515971Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.516213Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.516505Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.516708Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.516974Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.517282Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.517664Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.517960Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.518251Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:25.518491Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
