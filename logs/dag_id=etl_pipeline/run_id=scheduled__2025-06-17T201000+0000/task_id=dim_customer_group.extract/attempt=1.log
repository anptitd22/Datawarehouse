{"timestamp":"2025-06-17T20:10:04.442528","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-17T20:10:04.445956","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_pipeline.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-17T20:10:10.781017Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:10:10.781749Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:10:10.782237Z","level":"info","event":"Current task name:dim_customer_group.extract","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:10:10.782661Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-06-17T20:10:12.187683Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.451652Z","level":"error","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.876348Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2.5.2/cache","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.876914Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2.5.2/jars","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.880211Z","level":"error","event":"org.postgresql#postgresql added as a dependency","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.883863Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-5a2384ba-d7ba-4bc5-b438-226cde3a7a86;1.0","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:16.884448Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.404727Z","level":"error","event":"\tfound org.postgresql#postgresql;42.7.1 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.433240Z","level":"error","event":"\tfound org.checkerframework#checker-qual;3.41.0 in central","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.445628Z","level":"error","event":":: resolution report :: resolve 558ms :: artifacts dl 4ms","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.447010Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.448505Z","level":"error","event":"\torg.checkerframework#checker-qual;3.41.0 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.450346Z","level":"error","event":"\torg.postgresql#postgresql;42.7.1 from central in [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.452707Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.454018Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.455287Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.456569Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.457953Z","level":"error","event":"\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.459043Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.460436Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-5a2384ba-d7ba-4bc5-b438-226cde3a7a86","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.462318Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.464661Z","level":"error","event":"\t0 artifacts copied, 2 already retrieved (0kB/6ms)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:17.857682Z","level":"error","event":"25/06/17 20:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:18.196095Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:18.197025Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:18.197725Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:10:20.307976Z","level":"error","event":"25/06/17 20:10:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:12.788832Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:11:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:27.783025Z","level":"error","event":"25/06/17 20:11:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:42.933472Z","level":"error","event":"25/06/17 20:11:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:11:57.933493Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:11:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:13.065780Z","level":"error","event":"25/06/17 20:12:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:28.065854Z","level":"error","event":"25/06/17 20:12:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:43.196604Z","level":"error","event":"25/06/17 20:12:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:12:58.196977Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:12:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:13.363409Z","level":"error","event":"25/06/17 20:13:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:28.363545Z","level":"error","event":"25/06/17 20:13:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:43.511915Z","level":"error","event":"25/06/17 20:13:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:13:58.511879Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:13:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:13.656635Z","level":"error","event":"25/06/17 20:14:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:28.656533Z","level":"error","event":"25/06/17 20:14:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:43.808222Z","level":"error","event":"25/06/17 20:14:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:14:58.808073Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:14:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:13.964001Z","level":"error","event":"25/06/17 20:15:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:28.963959Z","level":"error","event":"25/06/17 20:15:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:44.103881Z","level":"error","event":"25/06/17 20:15:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:15:59.103706Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:15:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:14.239712Z","level":"error","event":"25/06/17 20:16:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:29.239638Z","level":"error","event":"25/06/17 20:16:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:44.405067Z","level":"error","event":"25/06/17 20:16:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:16:59.405129Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r25/06/17 20:16:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:14.557387Z","level":"error","event":"25/06/17 20:17:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:29.560754Z","level":"error","event":"25/06/17 20:17:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:40.964581Z","level":"error","event":"Server indicated the task shouldn't be running anymore. Terminating process","detail":{"detail":{"reason":"not_found","message":"Task Instance not found"}},"logger":"task"}
{"timestamp":"2025-06-17T20:17:41.044636Z","level":"error","event":"Task killed!","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.243841Z","level":"error","event":"25/06/17 20:17:41 ERROR FileFormatWriter: Aborting job 9dd99f47-41a5-4220-af5f-d03f0b0c91e1.","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.244681Z","level":"error","event":"org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.251895Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.254209Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.259108Z","level":"error","event":"\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.261481Z","level":"error","event":"\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.265250Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.282099Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.282845Z","level":"error","event":"\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.283616Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.287610Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.293318Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.296267Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.310768Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.319833Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.342554Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.354802Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$36(SparkContext.scala:704)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.358289Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.362145Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.364043Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.371438Z","level":"error","event":"\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.371947Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.372323Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.380882Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.384493Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:205)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.385209Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.387964Z","level":"error","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.388797Z","level":"error","event":"\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.389412Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.389846Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.390309Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.390656Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.390986Z","level":"error","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.391247Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:309)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.391498Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.391701Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.392119Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.392360Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.395164Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.395902Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.398596Z","level":"error","event":"\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.402162Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.402781Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.409958Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.410555Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.410990Z","level":"error","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.411321Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.419848Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.420334Z","level":"error","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.420697Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.420879Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.421042Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.421179Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.425209Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.425770Z","level":"error","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.426190Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.426511Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.426817Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.427142Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.429325Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.429985Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.430349Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.430765Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.431168Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.431525Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.431857Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.432197Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.432500Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.432870Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.433311Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.433581Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.433793Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.434059Z","level":"error","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.434297Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.434506Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.434828Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.435221Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.435536Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.435847Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.436268Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.436738Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.437054Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.437353Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.437705Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.437992Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.438461Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.438770Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.439040Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.439268Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.439502Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.439751Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.440006Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.440226Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.440457Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-06-17T20:17:41.440697Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
